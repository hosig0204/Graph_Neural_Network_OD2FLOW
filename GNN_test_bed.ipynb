{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports.\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchmetrics import MeanAbsolutePercentageError as MAPE\n",
    "from torch_geometric.utils import to_networkx\n",
    "from od_gnn_cls.gnn_dataset import *\n",
    "from od_gnn_cls.gnn_gcn import *\n",
    "from od_gnn_cls.gnn_gat import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device check.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Import dataset: InMemory Dataset, PyG graph data is already prepared.\n",
    "str_dir_dataset_root = \"dataset_history_pyg_inMemory\"       # Just include root directory.\n",
    "# Import InMemory dataset.\n",
    "dataset_od_flow = od_flow_graphs_inMemory(\n",
    "    root= \"dataset_history_pyg_inMemory\",\n",
    "    lst_path_graphs= []\n",
    ")\n",
    "# Sample data to extract dimension info.\n",
    "data_sample = dataset_od_flow[0]                            \n",
    "int_dim_node_features = int(data_sample.num_node_features)  # Node feature dimension.\n",
    "int_dim_node_out = int(data_sample.y.shape[1])              # Node output value dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our graph.\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it looks like..\n",
    "g = to_networkx(data_sample, to_undirected= True)\n",
    "nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input OD vectors. (Node features, [nrVehs])\n",
    "data_sample.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output link flows. (Node outputs, [nrVehs/hr])\n",
    "data_sample.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other characteristics.\n",
    "print(data_sample.num_nodes)\n",
    "print(data_sample.num_node_features)\n",
    "print(data_sample.num_edges)\n",
    "print(data_sample.num_features)\n",
    "print(data_sample.is_undirected())\n",
    "print(data_sample.has_self_loops())\n",
    "print(data_sample.has_isolated_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define size of datasets for test and train\n",
    "int_size_dataset = len(dataset_od_flow)\n",
    "float_rat_train = 0.8     # Sum of ratios should be 1.\n",
    "float_rat_test = 0.2 \n",
    "int_size_train = int(int_size_dataset*float_rat_train)\n",
    "int_size_test = int(int_size_dataset*float_rat_test)\n",
    "\n",
    "# Split original dataset into test and train datasets.\n",
    "dataset_train, dataset_test = random_split(dataset_od_flow, [int_size_train, int_size_test])\n",
    "\n",
    "# Print size information.\n",
    "print(\"Graph sets have been split.\")\n",
    "print(\"Total Graphs: {}\".format(int_size_dataset))\n",
    "print(\"   Train Graphs: {}\".format(int_size_train))\n",
    "print(\"   Test Graphs: {}\".format(int_size_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have batched dataset (surely from PyG, not basic Pytorch)\n",
    "int_size_batch = 32 # Some number as 2^x... (e.g. 32,64 ..)\n",
    "loaded_train = DataLoader(dataset_train, batch_size= int_size_batch, shuffle= True)\n",
    "loaded_test = DataLoader(dataset_test, batch_size= int_size_batch, shuffle= True)\n",
    "# Print out process.\n",
    "print(\"Data has been loaded. Batch Size: {}\".format(int_size_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before use main model, let's check if graph convolution is working.\n",
    "stupiud_test_model = gnn_GCN_CONV_test(14,2,14).to(device)\n",
    "stupiud_test_model.forward(data_sample.to(device))\n",
    "# As GCN utilize Laplacian matrix spectoral convolution,\n",
    "# there should be minus values and also diagonal elements shouldn't be all zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for training parameters should be reset before training.\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    model.train() # Activate drop-out layers.\n",
    "    \n",
    "    loss = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, data in enumerate(dataloader):\n",
    "        # Prediction from forward calculation.\n",
    "        # Loss term calculation.\n",
    "        data.to(device)\n",
    "        pred = model(data)\n",
    "        loss = loss_fn(pred, data.y)\n",
    "        # Back-propagation and optimization.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Reporting.\n",
    "        if batch % 10 == 0 : # For each 10 batchs.\n",
    "            loss_val = loss.item()\n",
    "            nr_used_data = int((batch * len(data.x)) / int_dim_node_features)\n",
    "            print(\"Loss: {loss:>.5f}  [{current:>5d}/{size:>5d}]\".format(loss=loss_val, current= nr_used_data, size= size))\n",
    "\n",
    "# Loop for test.\n",
    "@torch.no_grad()    # Context-manager that disabled gradient calculation.\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \n",
    "    model.eval() # Deactivate drop-out layer.\n",
    "    \n",
    "    # size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    clc_mape = MAPE().to(device)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        data.to(device)\n",
    "        pred = model(data)\n",
    "        test_loss += loss_fn(pred, data.y).item()\n",
    "        correct += clc_mape(pred, data.y)\n",
    "        # Below is for classification !!\n",
    "        # correct += (pred.argmax(1) == y).type(torch.float).sum().item() \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    \n",
    "    print(f\"Test Error: \\n MAPE: {(correct*100):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return [correct*100, test_loss]\n",
    "\n",
    "# Function for run training.\n",
    "def run_training(in_model, in_tot_epoch:int = 300, in_lr:int = 0.0005) -> pd.DataFrame:\n",
    "    \n",
    "    # Let your machine works.\n",
    "    start_time = time.time()                # Timer starts.\n",
    "    in_model.reset_parameters()             # Reset all parameters in the model.\n",
    "    loss_fn = torch.nn.MSELoss()            # Loss function: MSE\n",
    "    # Optimizer init.\n",
    "    optimizer = torch.optim.Adam(in_model.parameters(), lr=in_lr)\n",
    "    epochs = in_tot_epoch                   # Total number of iterations. NOTE:RECOMMEND ABOVE 300.\n",
    "    # Empty lists for stamps.\n",
    "    lst_mape = []       \n",
    "    lst_loss = []\n",
    "    lst_time = []\n",
    "\n",
    "    # Training Starts!\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(loaded_train, in_model, loss_fn, optimizer)\n",
    "        mape, loss = test_loop(loaded_test, in_model, loss_fn)\n",
    "        time_epoch = time.time() - start_time\n",
    "        lst_time.append(int(time_epoch))\n",
    "        lst_mape.append(float(mape))\n",
    "        lst_loss.append(float(loss))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # Keep training record.\n",
    "    len_hist_learn_tmp = len(lst_loss)\n",
    "    dic_hist_learn_tmp = {\n",
    "        \"Iteration\" : range(1, len_hist_learn_tmp + 1),\n",
    "        \"Time\": lst_time,\n",
    "        \"MSE_Loss\" : lst_loss,\n",
    "        \"MAPE\" : lst_mape\n",
    "    }\n",
    "    df_hist_learn_tmp = pd.DataFrame(dic_hist_learn_tmp)\n",
    "    \n",
    "    # Return training record.\n",
    "    return df_hist_learn_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK...stupid model says GCNConv layer is doing its job.\n",
    "# Let's build not that much stupid model. \n",
    "\n",
    "# GNN MODEL IMPORT\n",
    "# FIRST TRIAL: 2GCN + 1LIN layers, No BatchNorm, No Dropout.\n",
    "model_2GCN_1LIN = gnn_GCN_CONV_LIN(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_num_layers= 2, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_2GCN_1LIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU. \n",
    "# Um...I got my personal RTX3060 12GB. I'd say it's not for Gaming purpose :)...\n",
    "df_hist_learn_2GCN_1LIN = run_training(model_2GCN_1LIN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your learning history via last part of the dataframe.\n",
    "df_hist_learn_2GCN_1LIN.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN MODEL IMPORT\n",
    "# SECOND TRIAL: 2GAT + 1LIN layers, No BatchNorm, No Dropout.\n",
    "model_2GAT_1LIN = gnn_GAT_CONV_LIN(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_num_layers= 2, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_2GAT_1LIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU.\n",
    "df_hist_learn_2GAT_1LIN = run_training(model_2GAT_1LIN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result.\n",
    "df_hist_learn_2GAT_1LIN.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN MODEL IMPORT\n",
    "# THIRD TRIAL: 2GAT + 1LIN layers 0.2 Negative Slope, No BatchNorm, No Dropout.\n",
    "# Model importing with relevant arguments.\n",
    "model_2GAT_1LIN_NegSlope = gnn_GAT_CONV_LIN(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_neg_slope= 0.2, in_num_layers= 2, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_2GAT_1LIN_NegSlope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU.\n",
    "df_hist_learn_2GAT_1LIN_NegSlope = run_training(model_2GAT_1LIN_NegSlope, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result.\n",
    "df_hist_learn_2GAT_1LIN_NegSlope.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN MODEL IMPORT\n",
    "# FOURTH TRIAL: 2GCN + 2LIN layers, No BatchNorm, No Dropout.\n",
    "# Model importing with relevant arguments.\n",
    "model_2GCN_2LIN = gnn_GCN_CONV_LIN2(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_num_layers= 2, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_2GCN_2LIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU.\n",
    "df_hist_learn_2GCN_2LIN = run_training(model_2GCN_2LIN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result.\n",
    "df_hist_learn_2GCN_2LIN.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN MODEL IMPORT\n",
    "# FIFTH TRIAL: 2GATv2 + 1LIN layers, No BatchNorm, No Dropout.\n",
    "# Model importing with relevant arguments.\n",
    "model_2GATv2_1LIN = gnn_GATv2_CONV_LIN(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_num_layers= 2, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_2GATv2_1LIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU.\n",
    "df_hist_learn_2GATv2_1LIN = run_training(model_2GATv2_1LIN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result.\n",
    "df_hist_learn_2GATv2_1LIN.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model importing with relevant arguments.\n",
    "model_3GATv2_1LIN = gnn_GATv2_CONV_LIN(\n",
    "    in_dim_x= int_dim_node_features, in_dim_y= int_dim_node_out,\n",
    "    in_dim_hid= int_dim_node_features, in_num_layers= 3, \n",
    "    in_lc_norm= False, in_lc_dropout= False\n",
    ").to(device)\n",
    "\n",
    "# Print-out model spec.\n",
    "# Actual layer structure is not same as printed results!\n",
    "print(model_3GATv2_1LIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let your model works. Hope you have a good GPU.\n",
    "df_hist_learn_3GATv2_1LIN = run_training(model_3GATv2_1LIN, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result.\n",
    "df_hist_learn_3GATv2_1LIN.tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
